{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des librairies pyspark\n",
    "\n",
    "# Initialisation de Spark\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "#import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MNIST Classifier\") \\\n",
    "    .config('spark.sql.warehouse.dir', 'heart.csv') \\\n",
    "    .config('spark.executor.instances', 10) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du fichier mnist (partie train)\n",
    "\n",
    "\n",
    "fileNameTrain = 'heart800.csv'\n",
    "mnist_train = spark.read.csv(fileNameTrain, header=True)\n",
    "\n",
    "# Lecture du fichier mnist (partie test)\n",
    "fileNameTest = 'heart226.csv'\n",
    "mnist_test = spark.read.csv(fileNameTest, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ages: float (nullable = true)\n",
      " |-- sex: float (nullable = true)\n",
      " |-- cp: float (nullable = true)\n",
      " |-- trestbps: float (nullable = true)\n",
      " |-- chol: float (nullable = true)\n",
      " |-- fbs: float (nullable = true)\n",
      " |-- restecg: float (nullable = true)\n",
      " |-- thalach: float (nullable = true)\n",
      " |-- exang: float (nullable = true)\n",
      " |-- oldpeak: float (nullable = true)\n",
      " |-- slope: float (nullable = true)\n",
      " |-- ca: float (nullable = true)\n",
      " |-- thal: float (nullable = true)\n",
      " |-- target: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = ['ages', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
    "for col_name in numeric_columns:\n",
    "    mnist_train = mnist_train.withColumn(col_name, col(col_name).cast(FloatType()))\n",
    "\n",
    "\n",
    "#afficher le schema\n",
    "mnist_train.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of mnist_test after conversion:\n",
      "root\n",
      " |-- ages: float (nullable = true)\n",
      " |-- sex: float (nullable = true)\n",
      " |-- cp: float (nullable = true)\n",
      " |-- trestbps: float (nullable = true)\n",
      " |-- chol: float (nullable = true)\n",
      " |-- fbs: float (nullable = true)\n",
      " |-- restecg: float (nullable = true)\n",
      " |-- thalach: float (nullable = true)\n",
      " |-- exang: float (nullable = true)\n",
      " |-- oldpeak: float (nullable = true)\n",
      " |-- slope: float (nullable = true)\n",
      " |-- ca: float (nullable = true)\n",
      " |-- thal: float (nullable = true)\n",
      " |-- target: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Liste des colonnes à convertir en types numériques\n",
    "numeric_columns_test = ['ages', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
    "\n",
    "# Convertir les colonnes en types numériques\n",
    "for col_name in numeric_columns_test:\n",
    "    mnist_test = mnist_test.withColumn(col_name, col(col_name).cast(FloatType()))\n",
    "\n",
    "# Afficher le nouveau schéma\n",
    "print(\"Schema of mnist_test after conversion:\")\n",
    "mnist_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled Points (Training Set):\n",
      "+------+---------------------------------------------------------------------------+\n",
      "|target|features                                                                   |\n",
      "+------+---------------------------------------------------------------------------+\n",
      "|0.0   |[52.0,1.0,0.0,125.0,212.0,0.0,1.0,168.0,0.0,1.0,2.0,2.0,3.0]               |\n",
      "|0.0   |[53.0,1.0,0.0,140.0,203.0,1.0,0.0,155.0,1.0,3.0999999046325684,0.0,0.0,3.0]|\n",
      "|0.0   |[70.0,1.0,0.0,145.0,174.0,0.0,1.0,125.0,1.0,2.5999999046325684,0.0,0.0,3.0]|\n",
      "|0.0   |[61.0,1.0,0.0,148.0,203.0,0.0,1.0,161.0,0.0,0.0,2.0,1.0,3.0]               |\n",
      "|0.0   |[62.0,0.0,0.0,138.0,294.0,1.0,1.0,106.0,0.0,1.899999976158142,1.0,3.0,2.0] |\n",
      "|1.0   |(13,[0,3,4,7,9,10,12],[58.0,100.0,248.0,122.0,1.0,1.0,2.0])                |\n",
      "|0.0   |[58.0,1.0,0.0,114.0,318.0,0.0,2.0,140.0,0.0,4.400000095367432,0.0,3.0,1.0] |\n",
      "|0.0   |[55.0,1.0,0.0,160.0,289.0,0.0,0.0,145.0,1.0,0.800000011920929,1.0,1.0,3.0] |\n",
      "|0.0   |[46.0,1.0,0.0,120.0,249.0,0.0,0.0,144.0,0.0,0.800000011920929,2.0,0.0,3.0] |\n",
      "|0.0   |[54.0,1.0,0.0,122.0,286.0,0.0,0.0,116.0,1.0,3.200000047683716,1.0,2.0,2.0] |\n",
      "|1.0   |[71.0,0.0,0.0,112.0,149.0,0.0,1.0,125.0,0.0,1.600000023841858,1.0,0.0,2.0] |\n",
      "|0.0   |[43.0,0.0,0.0,132.0,341.0,1.0,0.0,136.0,1.0,3.0,1.0,0.0,3.0]               |\n",
      "|1.0   |[34.0,0.0,1.0,118.0,210.0,0.0,1.0,192.0,0.0,0.699999988079071,2.0,0.0,2.0] |\n",
      "|0.0   |[51.0,1.0,0.0,140.0,298.0,0.0,1.0,122.0,1.0,4.199999809265137,1.0,3.0,3.0] |\n",
      "|0.0   |[52.0,1.0,0.0,128.0,204.0,1.0,1.0,156.0,1.0,1.0,1.0,0.0,0.0]               |\n",
      "|1.0   |[34.0,0.0,1.0,118.0,210.0,0.0,1.0,192.0,0.0,0.699999988079071,2.0,0.0,2.0] |\n",
      "|1.0   |[51.0,0.0,2.0,140.0,308.0,0.0,0.0,142.0,0.0,1.5,2.0,1.0,2.0]               |\n",
      "|0.0   |[54.0,1.0,0.0,124.0,266.0,0.0,0.0,109.0,1.0,2.200000047683716,1.0,1.0,3.0] |\n",
      "|1.0   |[50.0,0.0,1.0,120.0,244.0,0.0,1.0,162.0,0.0,1.100000023841858,2.0,0.0,2.0] |\n",
      "|1.0   |[58.0,1.0,2.0,140.0,211.0,1.0,0.0,165.0,0.0,0.0,2.0,0.0,2.0]               |\n",
      "+------+---------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Labeled Points Test (Test Set):\n",
      "+------+----------------------------------------------------------------------------+\n",
      "|target|features                                                                    |\n",
      "+------+----------------------------------------------------------------------------+\n",
      "|1.0   |[43.0,1.0,0.0,115.0,303.0,0.0,1.0,181.0,0.0,1.2000000476837158,1.0,0.0,2.0] |\n",
      "|0.0   |[67.0,1.0,0.0,120.0,229.0,0.0,0.0,129.0,1.0,2.5999999046325684,1.0,2.0,3.0] |\n",
      "|1.0   |[63.0,1.0,3.0,145.0,233.0,1.0,0.0,150.0,0.0,2.299999952316284,0.0,0.0,1.0]  |\n",
      "|0.0   |[63.0,0.0,0.0,124.0,197.0,0.0,1.0,136.0,1.0,0.0,1.0,0.0,2.0]                |\n",
      "|0.0   |[52.0,1.0,0.0,112.0,230.0,0.0,1.0,160.0,0.0,0.0,2.0,1.0,2.0]                |\n",
      "|1.0   |[58.0,0.0,0.0,130.0,197.0,0.0,1.0,131.0,0.0,0.6000000238418579,1.0,0.0,2.0] |\n",
      "|1.0   |[53.0,1.0,0.0,142.0,226.0,0.0,0.0,111.0,1.0,0.0,2.0,0.0,3.0]                |\n",
      "|0.0   |[57.0,1.0,0.0,150.0,276.0,0.0,0.0,112.0,1.0,0.6000000238418579,1.0,1.0,1.0] |\n",
      "|1.0   |[44.0,1.0,2.0,130.0,233.0,0.0,1.0,179.0,1.0,0.4000000059604645,2.0,0.0,2.0] |\n",
      "|1.0   |[51.0,1.0,2.0,94.0,227.0,0.0,1.0,154.0,1.0,0.0,2.0,1.0,3.0]                 |\n",
      "|1.0   |[54.0,0.0,2.0,110.0,214.0,0.0,1.0,158.0,0.0,1.600000023841858,1.0,0.0,2.0]  |\n",
      "|0.0   |[40.0,1.0,0.0,110.0,167.0,0.0,0.0,114.0,1.0,2.0,1.0,0.0,3.0]                |\n",
      "|0.0   |[57.0,1.0,1.0,124.0,261.0,0.0,1.0,141.0,0.0,0.30000001192092896,2.0,0.0,3.0]|\n",
      "|0.0   |(13,[0,3,4,7,9,11,12],[62.0,140.0,268.0,160.0,3.5999999046325684,2.0,2.0])  |\n",
      "|0.0   |[53.0,1.0,0.0,140.0,203.0,1.0,0.0,155.0,1.0,3.0999999046325684,0.0,0.0,3.0] |\n",
      "|1.0   |[62.0,1.0,1.0,128.0,208.0,1.0,0.0,140.0,0.0,0.0,2.0,0.0,2.0]                |\n",
      "|1.0   |[58.0,1.0,2.0,105.0,240.0,0.0,0.0,154.0,1.0,0.6000000238418579,1.0,0.0,3.0] |\n",
      "|1.0   |[70.0,1.0,1.0,156.0,245.0,0.0,0.0,143.0,0.0,0.0,2.0,0.0,2.0]                |\n",
      "|1.0   |(13,[0,1,3,4,7,10,12],[45.0,1.0,115.0,260.0,185.0,2.0,2.0])                 |\n",
      "|1.0   |[42.0,1.0,3.0,148.0,244.0,0.0,0.0,178.0,0.0,0.800000011920929,2.0,2.0,2.0]  |\n",
      "+------+----------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Liste des colonnes à assembler (toutes sauf \"target\")\n",
    "input_cols = [col_name for col_name in mnist_train.columns if col_name != 'target']\n",
    "\n",
    "# Création du VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=input_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Transformation sur le jeu de données d'entraînement\n",
    "labeledPoints = assembler.transform(mnist_train)\n",
    "\n",
    "# Création des colonnes \"label\" et \"features\" pour le jeu de données d'entraînement\n",
    "labeledPoints = labeledPoints.select([ 'target', 'features'])\n",
    "\n",
    "# Transformation sur le jeu de données de test\n",
    "labeledPointsTest = assembler.transform(mnist_test)\n",
    "\n",
    "# Création des colonnes \"label\" et \"features\" pour le jeu de données de test\n",
    "labeledPointsTest = labeledPointsTest.select([ 'target', 'features'])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Labeled Points (Training Set):\")\n",
    "labeledPoints.show(truncate=False)\n",
    "\n",
    "print(\"Labeled Points Test (Test Set):\")\n",
    "labeledPointsTest.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- target: float (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+------+--------------------+\n",
      "|target|            features|\n",
      "+------+--------------------+\n",
      "|   0.0|[52.0,1.0,0.0,125...|\n",
      "|   0.0|[53.0,1.0,0.0,140...|\n",
      "|   0.0|[70.0,1.0,0.0,145...|\n",
      "|   0.0|[61.0,1.0,0.0,148...|\n",
      "|   0.0|[62.0,0.0,0.0,138...|\n",
      "|   1.0|(13,[0,3,4,7,9,10...|\n",
      "|   0.0|[58.0,1.0,0.0,114...|\n",
      "|   0.0|[55.0,1.0,0.0,160...|\n",
      "|   0.0|[46.0,1.0,0.0,120...|\n",
      "|   0.0|[54.0,1.0,0.0,122...|\n",
      "|   1.0|[71.0,0.0,0.0,112...|\n",
      "|   0.0|[43.0,0.0,0.0,132...|\n",
      "|   1.0|[34.0,0.0,1.0,118...|\n",
      "|   0.0|[51.0,1.0,0.0,140...|\n",
      "|   0.0|[52.0,1.0,0.0,128...|\n",
      "|   1.0|[34.0,0.0,1.0,118...|\n",
      "|   1.0|[51.0,0.0,2.0,140...|\n",
      "|   0.0|[54.0,1.0,0.0,124...|\n",
      "|   1.0|[50.0,0.0,1.0,120...|\n",
      "|   1.0|[58.0,1.0,2.0,140...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labeledPoints.printSchema()\n",
    "labeledPoints.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled Points (Training Set):\n",
      "+------+---------------------------------------------------------------------------+----------+\n",
      "|target|features                                                                   |labelIndex|\n",
      "+------+---------------------------------------------------------------------------+----------+\n",
      "|0.0   |[52.0,1.0,0.0,125.0,212.0,0.0,1.0,168.0,0.0,1.0,2.0,2.0,3.0]               |0         |\n",
      "|0.0   |[53.0,1.0,0.0,140.0,203.0,1.0,0.0,155.0,1.0,3.0999999046325684,0.0,0.0,3.0]|0         |\n",
      "|0.0   |[70.0,1.0,0.0,145.0,174.0,0.0,1.0,125.0,1.0,2.5999999046325684,0.0,0.0,3.0]|0         |\n",
      "|0.0   |[61.0,1.0,0.0,148.0,203.0,0.0,1.0,161.0,0.0,0.0,2.0,1.0,3.0]               |0         |\n",
      "|0.0   |[62.0,0.0,0.0,138.0,294.0,1.0,1.0,106.0,0.0,1.899999976158142,1.0,3.0,2.0] |0         |\n",
      "|1.0   |(13,[0,3,4,7,9,10,12],[58.0,100.0,248.0,122.0,1.0,1.0,2.0])                |1         |\n",
      "|0.0   |[58.0,1.0,0.0,114.0,318.0,0.0,2.0,140.0,0.0,4.400000095367432,0.0,3.0,1.0] |0         |\n",
      "|0.0   |[55.0,1.0,0.0,160.0,289.0,0.0,0.0,145.0,1.0,0.800000011920929,1.0,1.0,3.0] |0         |\n",
      "|0.0   |[46.0,1.0,0.0,120.0,249.0,0.0,0.0,144.0,0.0,0.800000011920929,2.0,0.0,3.0] |0         |\n",
      "|0.0   |[54.0,1.0,0.0,122.0,286.0,0.0,0.0,116.0,1.0,3.200000047683716,1.0,2.0,2.0] |0         |\n",
      "|1.0   |[71.0,0.0,0.0,112.0,149.0,0.0,1.0,125.0,0.0,1.600000023841858,1.0,0.0,2.0] |1         |\n",
      "|0.0   |[43.0,0.0,0.0,132.0,341.0,1.0,0.0,136.0,1.0,3.0,1.0,0.0,3.0]               |0         |\n",
      "|1.0   |[34.0,0.0,1.0,118.0,210.0,0.0,1.0,192.0,0.0,0.699999988079071,2.0,0.0,2.0] |1         |\n",
      "|0.0   |[51.0,1.0,0.0,140.0,298.0,0.0,1.0,122.0,1.0,4.199999809265137,1.0,3.0,3.0] |0         |\n",
      "|0.0   |[52.0,1.0,0.0,128.0,204.0,1.0,1.0,156.0,1.0,1.0,1.0,0.0,0.0]               |0         |\n",
      "|1.0   |[34.0,0.0,1.0,118.0,210.0,0.0,1.0,192.0,0.0,0.699999988079071,2.0,0.0,2.0] |1         |\n",
      "|1.0   |[51.0,0.0,2.0,140.0,308.0,0.0,0.0,142.0,0.0,1.5,2.0,1.0,2.0]               |1         |\n",
      "|0.0   |[54.0,1.0,0.0,124.0,266.0,0.0,0.0,109.0,1.0,2.200000047683716,1.0,1.0,3.0] |0         |\n",
      "|1.0   |[50.0,0.0,1.0,120.0,244.0,0.0,1.0,162.0,0.0,1.100000023841858,2.0,0.0,2.0] |1         |\n",
      "|1.0   |[58.0,1.0,2.0,140.0,211.0,1.0,0.0,165.0,0.0,0.0,2.0,0.0,2.0]               |1         |\n",
      "+------+---------------------------------------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Labeled Points Test (Test Set):\n",
      "+------+----------------------------------------------------------------------------+----------+\n",
      "|target|features                                                                    |labelIndex|\n",
      "+------+----------------------------------------------------------------------------+----------+\n",
      "|1.0   |[43.0,1.0,0.0,115.0,303.0,0.0,1.0,181.0,0.0,1.2000000476837158,1.0,0.0,2.0] |1         |\n",
      "|0.0   |[67.0,1.0,0.0,120.0,229.0,0.0,0.0,129.0,1.0,2.5999999046325684,1.0,2.0,3.0] |0         |\n",
      "|1.0   |[63.0,1.0,3.0,145.0,233.0,1.0,0.0,150.0,0.0,2.299999952316284,0.0,0.0,1.0]  |1         |\n",
      "|0.0   |[63.0,0.0,0.0,124.0,197.0,0.0,1.0,136.0,1.0,0.0,1.0,0.0,2.0]                |0         |\n",
      "|0.0   |[52.0,1.0,0.0,112.0,230.0,0.0,1.0,160.0,0.0,0.0,2.0,1.0,2.0]                |0         |\n",
      "|1.0   |[58.0,0.0,0.0,130.0,197.0,0.0,1.0,131.0,0.0,0.6000000238418579,1.0,0.0,2.0] |1         |\n",
      "|1.0   |[53.0,1.0,0.0,142.0,226.0,0.0,0.0,111.0,1.0,0.0,2.0,0.0,3.0]                |1         |\n",
      "|0.0   |[57.0,1.0,0.0,150.0,276.0,0.0,0.0,112.0,1.0,0.6000000238418579,1.0,1.0,1.0] |0         |\n",
      "|1.0   |[44.0,1.0,2.0,130.0,233.0,0.0,1.0,179.0,1.0,0.4000000059604645,2.0,0.0,2.0] |1         |\n",
      "|1.0   |[51.0,1.0,2.0,94.0,227.0,0.0,1.0,154.0,1.0,0.0,2.0,1.0,3.0]                 |1         |\n",
      "|1.0   |[54.0,0.0,2.0,110.0,214.0,0.0,1.0,158.0,0.0,1.600000023841858,1.0,0.0,2.0]  |1         |\n",
      "|0.0   |[40.0,1.0,0.0,110.0,167.0,0.0,0.0,114.0,1.0,2.0,1.0,0.0,3.0]                |0         |\n",
      "|0.0   |[57.0,1.0,1.0,124.0,261.0,0.0,1.0,141.0,0.0,0.30000001192092896,2.0,0.0,3.0]|0         |\n",
      "|0.0   |(13,[0,3,4,7,9,11,12],[62.0,140.0,268.0,160.0,3.5999999046325684,2.0,2.0])  |0         |\n",
      "|0.0   |[53.0,1.0,0.0,140.0,203.0,1.0,0.0,155.0,1.0,3.0999999046325684,0.0,0.0,3.0] |0         |\n",
      "|1.0   |[62.0,1.0,1.0,128.0,208.0,1.0,0.0,140.0,0.0,0.0,2.0,0.0,2.0]                |1         |\n",
      "|1.0   |[58.0,1.0,2.0,105.0,240.0,0.0,0.0,154.0,1.0,0.6000000238418579,1.0,0.0,3.0] |1         |\n",
      "|1.0   |[70.0,1.0,1.0,156.0,245.0,0.0,0.0,143.0,0.0,0.0,2.0,0.0,2.0]                |1         |\n",
      "|1.0   |(13,[0,1,3,4,7,10,12],[45.0,1.0,115.0,260.0,185.0,2.0,2.0])                 |1         |\n",
      "|1.0   |[42.0,1.0,3.0,148.0,244.0,0.0,0.0,178.0,0.0,0.800000011920929,2.0,2.0,2.0]  |1         |\n",
      "+------+----------------------------------------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Liste des colonnes à assembler (toutes sauf \"target\")\n",
    "input_cols = [col_name for col_name in mnist_train.columns if col_name != 'target']\n",
    "\n",
    "# Création du VectorAssembler\n",
    "assembler1 = VectorAssembler(\n",
    "    inputCols=input_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Transformation sur le jeu de données d'entraînement\n",
    "labeledPoints = assembler1.transform(mnist_train)\n",
    "\n",
    "# Création de la colonne \"labelIndex\"\n",
    "labeledPoints = labeledPoints.withColumn(\"labelIndex\", when(col(\"target\") == 0.0, 0).otherwise(1))\n",
    "\n",
    "# Création des colonnes \"target\", \"features\" et \"labelIndex\" pour le jeu de données d'entraînement\n",
    "labeledPoints = labeledPoints.select(['target', 'features', 'labelIndex'])\n",
    "\n",
    "# Transformation sur le jeu de données de test\n",
    "labeledPointsTest = assembler1.transform(mnist_test)\n",
    "\n",
    "# Création de la colonne \"labelIndex\" pour le jeu de données de test\n",
    "labeledPointsTest = labeledPointsTest.withColumn(\"labelIndex\", when(col(\"target\") == 0.0, 0).otherwise(1))\n",
    "\n",
    "# Création des colonnes \"target\", \"features\" et \"labelIndex\" pour le jeu de données de test\n",
    "labeledPointsTest = labeledPointsTest.select(['target', 'features', 'labelIndex'])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Labeled Points (Training Set):\")\n",
    "labeledPoints.show(truncate=False)\n",
    "\n",
    "print(\"Labeled Points Test (Test Set):\")\n",
    "labeledPointsTest.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(target=0.0, features=DenseVector([52.0, 1.0, 0.0, 125.0, 212.0, 0.0, 1.0, 168.0, 0.0, 1.0, 2.0, 2.0, 3.0]), labelIndex=0)\n"
     ]
    }
   ],
   "source": [
    "print(labeledPoints.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------------------------------------+\n",
      "|target|prediction|probability                              |\n",
      "+------+----------+-----------------------------------------+\n",
      "|1.0   |1.0       |[0.3964653209487039,0.6035346790512961]  |\n",
      "|0.0   |0.0       |[0.993737574696525,0.006262425303474983] |\n",
      "|1.0   |1.0       |[0.21395375505566033,0.7860462449443397] |\n",
      "|0.0   |1.0       |[0.30852682276643284,0.6914731772335672] |\n",
      "|0.0   |1.0       |[0.34025853074659984,0.6597414692534002] |\n",
      "|1.0   |1.0       |[0.19881077542777215,0.8011892245722279] |\n",
      "|1.0   |0.0       |[0.9027991070450547,0.09720089295494527] |\n",
      "|0.0   |0.0       |[0.9050606235793398,0.0949393764206602]  |\n",
      "|1.0   |1.0       |[0.1121250493906999,0.8878749506093001]  |\n",
      "|1.0   |1.0       |[0.3599667783562478,0.6400332216437522]  |\n",
      "|1.0   |1.0       |[0.034128315062532395,0.9658716849374676]|\n",
      "|0.0   |0.0       |[0.952211646320816,0.04778835367918399]  |\n",
      "|0.0   |1.0       |[0.3899118070653097,0.6100881929346903]  |\n",
      "|0.0   |0.0       |[0.9068748508998016,0.09312514910019842] |\n",
      "|0.0   |0.0       |[0.983929084021688,0.016070915978312006] |\n",
      "|1.0   |1.0       |[0.2520905858774775,0.7479094141225224]  |\n",
      "|1.0   |0.0       |[0.5463774168038653,0.4536225831961347]  |\n",
      "|1.0   |1.0       |[0.3282935182074854,0.6717064817925146]  |\n",
      "|1.0   |1.0       |[0.18860328334074877,0.8113967166592513] |\n",
      "|1.0   |1.0       |[0.14727667026344046,0.8527233297365595] |\n",
      "+------+----------+-----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Définition des classifieurs\n",
    "classifiers = [\n",
    "    RandomForestClassifier(labelCol=\"labelIndex\", featuresCol=\"features\", impurity='gini', maxBins=32),\n",
    "    LogisticRegression(labelCol=\"labelIndex\", featuresCol=\"features\", maxIter=10, regParam=0.01)\n",
    "]\n",
    "\n",
    "# Utilisation d'un classifieur spécifique (Logistic Regression dans cet exemple)\n",
    "classifier1 = LogisticRegression(labelCol=\"labelIndex\", featuresCol=\"features\", maxIter=10, regParam=0.01)\n",
    "\n",
    "# Utilisation de la pipeline pour entraîner le classifieur\n",
    "pipeline1 = Pipeline(stages=[classifier1])\n",
    "\n",
    "\n",
    "\n",
    "model1= pipeline1.fit(labeledPoints)\n",
    "\n",
    "\n",
    "# Utilisation du modèle pour prédire sur le jeu de données de test\n",
    "predictions1 = model1.transform(labeledPointsTest)\n",
    "\n",
    "\n",
    "# Affichage des résultats\n",
    "predictions1.select(\"target\", \"prediction\", \"probability\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------------------------------+\n",
      "|target|prediction|probability                             |\n",
      "+------+----------+----------------------------------------+\n",
      "|1.0   |1.0       |[0.19776177658418473,0.8022382234158153]|\n",
      "|0.0   |0.0       |[0.9092364162064346,0.09076358379356539]|\n",
      "|1.0   |1.0       |[0.2353210042318515,0.7646789957681486] |\n",
      "|0.0   |1.0       |[0.47898255634096926,0.5210174436590307]|\n",
      "|0.0   |0.0       |[0.5425442965937375,0.4574557034062625] |\n",
      "|1.0   |1.0       |[0.2017132952500681,0.7982867047499319] |\n",
      "|1.0   |0.0       |[0.6450940697437775,0.35490593025622247]|\n",
      "|0.0   |0.0       |[0.9429006210908591,0.05709937890914095]|\n",
      "|1.0   |1.0       |[0.10349577070822931,0.8965042292917706]|\n",
      "|1.0   |1.0       |[0.23509559685724674,0.7649044031427533]|\n",
      "|1.0   |1.0       |[0.08373830567449896,0.916261694325501] |\n",
      "|0.0   |0.0       |[0.9456514709711898,0.05434852902881008]|\n",
      "|0.0   |1.0       |[0.42060091031993957,0.5793990896800604]|\n",
      "|0.0   |0.0       |[0.7554161945787815,0.24458380542121852]|\n",
      "|0.0   |0.0       |[0.9034658681252112,0.09653413187478874]|\n",
      "|1.0   |1.0       |[0.11465171734818905,0.8853482826518109]|\n",
      "|1.0   |1.0       |[0.2652416951345961,0.7347583048654038] |\n",
      "|1.0   |1.0       |[0.15234739605470496,0.8476526039452951]|\n",
      "|1.0   |1.0       |[0.1686988960812724,0.8313011039187277] |\n",
      "|1.0   |1.0       |[0.17999392625433516,0.8200060737456648]|\n",
      "+------+----------+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Définition des classifieurs\n",
    "classifiers = [\n",
    "    RandomForestClassifier(labelCol=\"labelIndex\", featuresCol=\"features\", impurity='gini', maxBins=32),\n",
    "    LogisticRegression(labelCol=\"labelIndex\", featuresCol=\"features\", maxIter=10, regParam=0.01)\n",
    "]\n",
    "classifier2 = RandomForestClassifier(labelCol=\"labelIndex\", featuresCol=\"features\", impurity='gini', maxBins=32)\n",
    "pipeline2 =Pipeline(stages=[classifier2])\n",
    "model2= pipeline2.fit(labeledPoints)\n",
    "predictions2 = model2.transform(labeledPointsTest)\n",
    "predictions2.select(\"target\", \"prediction\", \"probability\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------------------------------+\n",
      "|target|prediction|probability                             |\n",
      "+------+----------+----------------------------------------+\n",
      "|1.0   |1.0       |[0.09085915055370573,0.9091408494462943]|\n",
      "|0.0   |0.0       |[0.9354631521256775,0.06453684787432246]|\n",
      "|1.0   |1.0       |[0.14266500952890523,0.8573349904710947]|\n",
      "|0.0   |0.0       |[0.7640411747336352,0.2359588252663648] |\n",
      "|0.0   |0.0       |[0.9064476959940395,0.09355230400596048]|\n",
      "|1.0   |1.0       |[0.10203035191717273,0.8979696480828273]|\n",
      "|1.0   |1.0       |[0.265716372628513,0.734283627371487]   |\n",
      "|0.0   |0.0       |[0.9346020546165054,0.06539794538349464]|\n",
      "|1.0   |1.0       |[0.05792650586679596,0.9420734941332041]|\n",
      "|1.0   |1.0       |[0.10302216708110305,0.896977832918897] |\n",
      "|1.0   |1.0       |[0.06335741157406234,0.9366425884259376]|\n",
      "|0.0   |0.0       |[0.9300483363838002,0.06995166361619976]|\n",
      "|0.0   |0.0       |[0.8497769438988433,0.15022305610115672]|\n",
      "|0.0   |0.0       |[0.9204324113094544,0.07956758869054559]|\n",
      "|0.0   |0.0       |[0.9713096649334505,0.0286903350665495] |\n",
      "|1.0   |1.0       |[0.1735257557649658,0.8264742442350342] |\n",
      "|1.0   |1.0       |[0.12979752728220323,0.8702024727177968]|\n",
      "|1.0   |1.0       |[0.22071594821696328,0.7792840517830367]|\n",
      "|1.0   |1.0       |[0.08412814583204949,0.9158718541679505]|\n",
      "|1.0   |1.0       |[0.11802416494537206,0.881975835054628] |\n",
      "+------+----------+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "# Définition des classifieurs\n",
    "gbt = GBTClassifier(labelCol=\"labelIndex\", featuresCol=\"features\", maxDepth=5, maxBins=32, maxIter=10)\n",
    "\n",
    "# Création du pipeline avec XGBoost\n",
    "pipeline_xgboost = Pipeline(stages=[\n",
    "    gbt\n",
    "])\n",
    "\n",
    "# Entraînement du modèle\n",
    "model_xgboost = pipeline_xgboost.fit(labeledPoints)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "predictions_xgboost = model_xgboost.transform(labeledPointsTest)\n",
    "\n",
    "# Affichage des résultats\n",
    "predictions_xgboost.select(\"target\", \"prediction\", \"probability\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.9424778761061947\n",
      "Test set f1 = 0.9424271719951054\n",
      "Test set weightedPrecision = 0.9453769410586075\n",
      "Test set weightedRecall = 0.9424778761061947\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "predictionsAndLabels_xgboost=predictions_xgboost.select(\"labelIndex\", \"prediction\")\n",
    "# Évaluateur d'accuracy\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator_accuracy.evaluate(predictionsAndLabels_xgboost)\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n",
    "\n",
    "# Évaluateur de F1\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator_f1.evaluate(predictionsAndLabels_xgboost)\n",
    "print(\"Test set f1 = \" + str(f1))\n",
    "\n",
    "# Évaluateur de weightedPrecision\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator_precision.evaluate(predictionsAndLabels_xgboost)\n",
    "print(\"Test set weightedPrecision = \" + str(precision))\n",
    "\n",
    "# Évaluateur de weightedRecall\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator_recall.evaluate(predictionsAndLabels_xgboost)\n",
    "print(\"Test set weightedRecall = \" + str(recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Test set accuracy = 0.8008849557522124\n",
      "Test set weightedPrecision = 0.8040996161099889\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# Sélection des colonnes pertinentes pour l'évaluation\n",
    "predictionAndLabels_1 = predictions1.select(\"labelIndex\", \"prediction\")\n",
    "\n",
    "# Évaluation\n",
    "evaluator_lr = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_lr = evaluator_lr.evaluate(predictionAndLabels_1)\n",
    "\n",
    "print(\"RandomForest Test set accuracy = \" + str(accuracy_lr))\n",
    "\n",
    "\n",
    "# Évaluateur de weightedPrecision\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator_precision.evaluate(predictionAndLabels_1)\n",
    "print(\"Test set weightedPrecision = \" + str(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Test set accuracy = 0.8584070796460177\n",
      "Test set weightedPrecision = 0.8595520470977158\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# Sélection des colonnes pertinentes pour l'évaluation\n",
    "predictionAndLabels_2 = predictions2.select(\"labelIndex\", \"prediction\")\n",
    "\n",
    "# Évaluation\n",
    "evaluator_rf = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_rf = evaluator_rf.evaluate(predictionAndLabels_2)\n",
    "\n",
    "print(\"RandomForest Test set accuracy = \" + str(accuracy_rf))\n",
    "\n",
    "\n",
    "# Évaluateur de weightedPrecision\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator_precision.evaluate(predictionAndLabels_2)\n",
    "print(\"Test set weightedPrecision = \" + str(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "hist() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\OneDrive\\Bureau\\projet big data\\project1Test.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Bureau/projet%20big%20data/project1Test.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mcsv(file_path, header\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, inferSchema\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Bureau/projet%20big%20data/project1Test.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Utiliser pyspark_dist_explore pour créer un histogramme\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Bureau/projet%20big%20data/project1Test.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m hist_data \u001b[39m=\u001b[39m hist(df, bins\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, color\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mskyblue\u001b[39;49m\u001b[39m'\u001b[39;49m, edgecolor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mblack\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Bureau/projet%20big%20data/project1Test.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Extraire les données pour Matplotlib\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Bureau/projet%20big%20data/project1Test.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m bins \u001b[39m=\u001b[39m hist_data[\u001b[39m'\u001b[39m\u001b[39mbin_edges\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: hist() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark_dist_explore import hist\n",
    "\n",
    "# Créer une session Spark\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "# Charger le fichier CSV dans un DataFrame PySpark\n",
    "file_path = 'heart.csv'\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Utiliser pyspark_dist_explore pour créer un histogramme\n",
    "hist_data = hist(df, bins=20, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Extraire les données pour Matplotlib\n",
    "bins = hist_data['bin_edges'][0]\n",
    "freq = hist_data['bin_values'][0]\n",
    "\n",
    "# Tracer l'histogramme avec Matplotlib\n",
    "plt.bar(bins[:-1], freq, width=bins[1]-bins[0], color='skyblue', edgecolor='black')\n",
    "plt.title('Répartition de l\\'âge en fonction de la maladie cardiaque')\n",
    "plt.xlabel('Âge')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
